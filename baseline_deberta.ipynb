{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "baseline-deberta.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "background_execution": "on",
      "mount_file_id": "https://github.com/xiaoyufan/nbme/blob/main/baseline_deberta.ipynb",
      "authorship_tag": "ABX9TyNhXyMAKE9pV0DtYNio13wd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xiaoyufan/nbme/blob/main/baseline_deberta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NBME Baseline DeBERTa"
      ],
      "metadata": {
        "id": "XoauZTv1ekis"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configurations"
      ],
      "metadata": {
        "id": "jWPOFAMmescu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "  batch_size = 16\n",
        "  device_name = 'gpu'\n",
        "  epochs = 1\n",
        "  input_dir = '/content/drive/MyDrive/CS7150 Deep Learning Project/Dataset/Preprocessed'\n",
        "  learning_rate = 1e-4\n",
        "  mode = 'dev'\n",
        "  model = 'microsoft/deberta-base'\n",
        "  output_dir = '/content/drive/MyDrive/CS7150 Deep Learning Project/Dataset/Output'\n",
        "  sequence_max_length = 466\n",
        "  tpu_cores = 1"
      ],
      "metadata": {
        "id": "i2tVkqRCK-bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Packages"
      ],
      "metadata": {
        "id": "8PAFcMWfep6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ipython-autotime\n",
        "%load_ext autotime"
      ],
      "metadata": {
        "id": "pVKbksi7x9Ps",
        "outputId": "0e1fc2be-9d43-43ab-a29d-c71aad4ad909",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipython-autotime in /usr/local/lib/python3.7/dist-packages (0.3.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipython-autotime) (5.5.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (1.0.18)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (2.6.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (5.1.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (57.4.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.8.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->ipython-autotime) (0.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fevyPYAS06FJ"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install tokenizers\n",
        "\n",
        "!pip install --upgrade git+https://github.com/xiaoyufan/nbme.git@e8aadf2b3fe5faa8bf1b3e884a4d2f4d7fbd026b"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from ast import literal_eval\n",
        "from nbme_utils.location import locations_to_spans, spans_to_locations\n",
        "from nbme_utils.prediction import logits_to_spans\n",
        "from transformers import AutoConfig, AutoModel, AutoTokenizer, PreTrainedTokenizer\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "8yvdXJKZFje_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TPU"
      ],
      "metadata": {
        "id": "HM32y2S4kQs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if Config.device_name == 'tpu':\n",
        "  assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'"
      ],
      "metadata": {
        "id": "wcVIbMFxkZJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cloud-tpu-client==0.10 torch==1.11.0 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-1.11-cp37-cp37m-linux_x86_64.whl"
      ],
      "metadata": {
        "id": "o4YNyQcrkUQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if Config.device_name == 'tpu':\n",
        "  import torch_xla.core.xla_model as xm\n",
        "  import torch_xla.distributed.parallel_loader as pl\n",
        "  import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "  \n",
        "  from torch.utils.data.distributed import DistributedSampler"
      ],
      "metadata": {
        "id": "Zy8T_2LmsIj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer"
      ],
      "metadata": {
        "id": "PWSc9tvyig71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(Config.model)\n",
        "tokenizer.save_pretrained(f'{Config.output_dir}/tokenizer')"
      ],
      "metadata": {
        "id": "1Pwdt6Qaij_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading"
      ],
      "metadata": {
        "id": "CJU893wUh3mT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining Dataset"
      ],
      "metadata": {
        "id": "e35rBE3XjzYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(f'{Config.input_dir}/train.csv')\n",
        "valid = pd.read_csv(f'{Config.input_dir}/validate.csv')\n",
        "test = pd.read_csv(f'{Config.input_dir}/test.csv')\n",
        "train.shape, valid.shape, test.shape"
      ],
      "metadata": {
        "id": "dD7OvlwAh4wO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if Config.mode == 'dev':\n",
        "  train = train.sample(n=400, random_state=0).reset_index(drop=True)\n",
        "  valid = valid.sample(n=100, random_state=0).reset_index(drop=True)\n",
        "train.shape, valid.shape, test.shape"
      ],
      "metadata": {
        "id": "xdAsvHtDiNw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "id": "kHFyvx81nxVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.head()"
      ],
      "metadata": {
        "id": "3LXOWx6bUEzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_labels(encoded: PreTrainedTokenizer, sample: pd.DataFrame):\n",
        "  labels = torch.zeros(len(encoded['input_ids']))\n",
        "\n",
        "  location_spans = locations_to_spans(literal_eval(sample['location']))\n",
        "\n",
        "  for idx, (seq_id, offsets)in enumerate(zip(encoded['sequence_ids'],\n",
        "                                             encoded['offset_mapping'])):\n",
        "    # None for special tokens added around or between sequences,\n",
        "    # 0 for tokens corresponding to words in the first sequence,\n",
        "    # 1 for tokens corresponding to words in the second sequence when a pair of sequences was jointly encoded.\n",
        "    # Labels are generated from patient notes, which are encoded as the first sequence.\n",
        "    if seq_id != 0:\n",
        "      labels[idx] = -1\n",
        "      continue\n",
        "\n",
        "    subtoken_start, subtoken_end = offsets\n",
        "\n",
        "    if any([subtoken_start >= location_start and subtoken_end <= location_end\n",
        "            for location_start, location_end in location_spans]):\n",
        "      labels[idx] = 1\n",
        "\n",
        "  return labels\n",
        "\n",
        "class NBMEDataset(Dataset):\n",
        "  def __init__(self, data: pd.DataFrame, tokenizer: AutoTokenizer, config: Config,\n",
        "               testing: bool = False):\n",
        "    self.data = data\n",
        "    self.tokenizer = tokenizer\n",
        "    self.config = config\n",
        "    self.testing = testing\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx: int):\n",
        "    sample = self.data.iloc[idx]\n",
        "\n",
        "    encoded = self.tokenizer(\n",
        "      sample['pn_history'],\n",
        "      sample['feature_text'],\n",
        "      # TODO: Compute max length of sequences\n",
        "      max_length=self.config.sequence_max_length,\n",
        "      padding='max_length',\n",
        "      return_offsets_mapping=True,\n",
        "    )\n",
        "    encoded['sequence_ids'] = np.array(encoded.sequence_ids()).astype('float16')\n",
        "\n",
        "    x = {k: torch.tensor(v, dtype=torch.long) for k, v in encoded.items()}\n",
        "\n",
        "    if self.testing:\n",
        "      return x\n",
        "\n",
        "    y_true = generate_labels(encoded, sample)\n",
        "    return x, y_true"
      ],
      "metadata": {
        "id": "Gv_EALppj4VC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = NBMEDataset(train, tokenizer, Config)\n",
        "valid_dataset = NBMEDataset(valid, tokenizer, Config)\n",
        "test_dataset = NBMEDataset(test, tokenizer, Config, testing=True)"
      ],
      "metadata": {
        "id": "zAGgrY6H2Sq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "PzznXRtpOYaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NBMEDebertaBaseline(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    config = AutoConfig.from_pretrained(Config.model, output_hidden_states=True)\n",
        "    self.model = AutoModel.from_pretrained(Config.model, config=config)\n",
        "\n",
        "    self.fc = nn.Linear(config.hidden_size, 1)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    hidden_states = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    logits = self.fc(hidden_states[0])\n",
        "    logits = torch.sigmoid(logits)\n",
        "    logits = logits.squeeze(-1)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "6XiPIjKPOZeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NBMEDebertaBaseline()"
      ],
      "metadata": {
        "id": "4c0UimsMUS9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "Yg0_Tq_qSlKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCEWithLogitsLoss(reduction = \"none\")\n",
        "\n",
        "def loss_fn(y_pred, y_true):\n",
        "  loss = criterion(y_pred, y_true)\n",
        "  # loss = torch.masked_select(loss, y_true >= 0).mean()\n",
        "  loss = loss.mean()\n",
        "  return loss"
      ],
      "metadata": {
        "id": "0-9krz4uK4Wy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training on GPU/CPU"
      ],
      "metadata": {
        "id": "sEqv4-0tbmgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fn(model, device, data_loader):\n",
        "  optimizer = torch.optim.AdamW(model.parameters(), lr=Config.learning_rate)\n",
        "\n",
        "  all_loss = []\n",
        "\n",
        "  for x, y_true in tqdm(data_loader):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    input_ids = x['input_ids'].to(device)\n",
        "    attention_mask = x['attention_mask'].to(device)\n",
        "    y_true = y_true.to(device)\n",
        "\n",
        "    logits = model(input_ids, attention_mask)\n",
        "    loss = loss_fn(logits, y_true)\n",
        "    all_loss.append(loss)\n",
        "\n",
        "    loss.backward()\n",
        "    \n",
        "    if Config.divice_name == 'tpu':\n",
        "      xm.optimizer_step(optimizer)\n",
        "    else:\n",
        "      optimizer.step()\n",
        "\n",
        "def valid_fn(model, device, data_loader):\n",
        "  model.eval()\n",
        "\n",
        "  true_spans = []\n",
        "  all_logits = []\n",
        "  all_offsets = []\n",
        "  all_sequence_ids = []\n",
        "  \n",
        "  for x, y_true in tqdm(data_loader):\n",
        "    true_spans.append(y_true)\n",
        "    \n",
        "    input_ids = x['input_ids'].to(device)\n",
        "    attention_mask = x['attention_mask'].to(device)\n",
        "\n",
        "    logits = model(input_ids, attention_mask)\n",
        "\n",
        "    all_logits.append(logits.detach().cpu().numpy())\n",
        "    all_offsets.append(x['offset_mapping'].numpy())\n",
        "    all_sequence_ids.append(x['sequence_ids'].numpy())\n",
        "  \n",
        "  all_logits = np.concatenate(all_logits, axis=0)\n",
        "  all_offsets = np.concatenate(all_offsets, axis=0)\n",
        "  all_sequence_ids = np.concatenate(all_sequence_ids, axis=0)\n",
        "\n",
        "  pred_spans = logits_to_spans(all_logits, all_offsets, all_sequence_ids)\n",
        "  print(all_logits.shape, pred_spans.shape, true_spans.shape)"
      ],
      "metadata": {
        "id": "j3_PD6hISmJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training on GPU"
      ],
      "metadata": {
        "id": "wlUlOFeIbp0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_non_tpu():\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  print(f'Using {device}')\n",
        "\n",
        "  model.to(device)\n",
        "\n",
        "  train_loader = DataLoader(\n",
        "      train_dataset,\n",
        "      batch_size=Config.batch_size,\n",
        "      shuffle=True)\n",
        "  valid_loader = DataLoader(\n",
        "      valid_dataset,\n",
        "      batch_size=Config.batch_size,\n",
        "      shuffle=False)\n",
        "\n",
        "  for epoch in range(Config.epochs):\n",
        "    train_fn(model, device, train_loader)\n",
        "    valid_fn(model, device, valid_loader)\n",
        "\n",
        "if Config.device_name != 'tpu':\n",
        "  run_non_tpu()"
      ],
      "metadata": {
        "id": "tFcCvtu4UfLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_tpu(index):\n",
        "  device = xm.xla_device()\n",
        "  print(f'Process {index} is using {xm.xla_real_devices([str(device)])[0]}')\n",
        "\n",
        "  model.to(device)\n",
        "\n",
        "  train_sampler = DistributedSampler(\n",
        "    train_dataset,\n",
        "    num_replicas=xm.xrt_world_size(),\n",
        "    rank=xm.get_ordinal(),\n",
        "    shuffle=True)\n",
        "  valid_sampler = DistributedSampler(\n",
        "    valid_dataset,\n",
        "    num_replicas=xm.xrt_world_size(),\n",
        "    rank=xm.get_ordinal(),\n",
        "    shuffle=False)\n",
        "\n",
        "  train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=Config.batch_size,\n",
        "    sampler=train_sampler,\n",
        "    drop_last=True)\n",
        "  valid_loader = DataLoader(\n",
        "    valid_dataset,\n",
        "    batch_size=Config.batch_size,\n",
        "    sampler=valid_sampler,\n",
        "    drop_last=True)\n",
        "\n",
        "  train_loader = pl.MpDeviceLoader(train_loader, device)\n",
        "  valid_loader = pl.MpDeviceLoader(valid_loader, device)\n",
        "\n",
        "  for epoch in range(Config.epochs):\n",
        "    train_fn(model, device, train_loader)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      valid_fn(model, device, valid_loader)\n",
        "\n",
        "  # Barrier to prevent master from exiting before workers connect.\n",
        "  xm.rendezvous('init')\n",
        "\n",
        "if Config.device_name == 'tpu':\n",
        "  xmp.spawn(run_tpu, args=(), nprocs=Config.tpu_cores, start_method='fork')"
      ],
      "metadata": {
        "id": "PSBjR0aXLxsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "3hycpJFVg0n9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_fn(model, data_loader):\n",
        "  device = xm.xla_device()\n",
        "  \n",
        "  model.eval()\n",
        "\n",
        "  all_logits = []\n",
        "  all_offsets = []\n",
        "  all_sequence_ids = []\n",
        "  \n",
        "  for x in tqdm(data_loader):\n",
        "    input_ids = x['input_ids'].to(device)\n",
        "    attention_mask = x['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      logits = model(input_ids, attention_mask)\n",
        "\n",
        "    all_logits.append(logits.detach().cpu().numpy())\n",
        "    all_offsets.append(x['offset_mapping'].numpy())\n",
        "    all_sequence_ids.append(x['sequence_ids'].numpy())\n",
        "  \n",
        "  all_logits = np.concatenate(all_logits, axis=0)\n",
        "  all_offsets = np.concatenate(all_offsets, axis=0)\n",
        "  all_sequence_ids = np.concatenate(all_sequence_ids, axis=0)\n",
        "\n",
        "  all_spans = logits_to_spans(all_logits, all_offsets, all_sequence_ids)\n",
        "  locations = [spans_to_locations(spans) for spans in all_spans]\n",
        "  return locations"
      ],
      "metadata": {
        "id": "iJnrh4_3g2Nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_fn(model)"
      ],
      "metadata": {
        "id": "rtLcmapYc0p4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}